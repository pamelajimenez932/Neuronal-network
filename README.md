
This project implements a simple artificial neural network from scratch in Python using only NumPy, designed to solve the classic XOR logic gate problem. The network architecture includes an input layer with two neurons, one hidden layer with ten neurons, and an output layer with a single neuron. Training is performed through forward propagation, backpropagation, and gradient descent, without relying on high-level machine learning libraries like TensorFlow or PyTorch. Over 10,000 training epochs, the model adjusts its weights and biases to minimize the mean squared error (MSE), which is visualized in a convergence plot. After training, the network is tested on all possible XOR inputs to demonstrate that it has successfully learned the expected behavior. This project provides a hands-on understanding of the inner workings of neural networks and serves as a strong foundation for learning more advanced machine learning concepts.# Neuronal-network
